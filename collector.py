"""
ë°©íƒ„ ëª¨ë“œ ë°ì´í„° ìˆ˜ì§‘ê¸° (Bulletproof Collector)
- ë³´ì•ˆ ìš°íšŒ ì‹¬í™”: ì„¸ì…˜ ìœ ì§€, ëœë¤ ë”œë ˆì´, ìµœì‹  í—¤ë”
- ê³„ì¸µì  ì˜ˆì™¸ ì²˜ë¦¬: ëª¨ë“  ì—ëŸ¬ ìƒí™© ëŒ€ì‘
- ìš°ì•„í•œ ì‹¤íŒ¨: í¬ë˜ì‹œ ì—†ì´ ì¹œì ˆí•œ ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥
"""

import os
import sys  # <--- [ì¤‘ìš”] ì•„ê¹Œ ì—ëŸ¬ë¥¼ í•´ê²°í•˜ëŠ” í•µì‹¬ ì—´ì‡ ì…ë‹ˆë‹¤!
import time
import random
import logging
from pathlib import Path

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import cloudscraper
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('collector.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =========================
# ì „ì—­ ì„¤ì •
# =========================

# í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ì ¸ì™€ì„œ 'university_data' í´ë” ê²½ë¡œ í™•ì •
current_dir = Path(__file__).parent.absolute()
data_dir = current_dir / "university_data"

# í´ë” ê°•ì œ ìƒì„± ë° ê¶Œí•œ í™•ì¸
try:
    data_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"ë°ì´í„° ì €ì¥ ê²½ë¡œ í™•ë³´: {data_dir}")
except Exception as e:
    logger.warning(f"í´ë” ìƒì„± ì¤‘ ê²½ê³  (ë¬´ì‹œ ê°€ëŠ¥): {e}")

# í•™êµ í™ˆí˜ì´ì§€ ë©”ì¸ ì£¼ì†Œ
KMOU_MAIN_URL = "https://www.kmou.ac.kr"

# ìµœì‹  Chrome User-Agent (2025ë…„ 1ì›” ê¸°ì¤€)
LATEST_CHROME_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/131.0.0.0 Safari/537.36"
)

# =========================
# CloudScraper ì„¸ì…˜ ì´ˆê¸°í™” (ì¿ í‚¤ ìœ ì§€)
# =========================

def create_scraper_session():
    """
    CloudScraper ì„¸ì…˜ì„ ìƒì„±í•˜ì—¬ ì¿ í‚¤ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
    """
    try:
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            },
            delay=random.uniform(1, 2)  # ì´ˆê¸° ë”œë ˆì´
        )
        logger.info("CloudScraper ì„¸ì…˜ ìƒì„± ì™„ë£Œ")
        return scraper
    except Exception as e:
        logger.error(f"CloudScraper ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ì „ì—­ ì„¸ì…˜ (ì¿ í‚¤ ìœ ì§€)
scraper_session = create_scraper_session()

# =========================
# í—¤ë” ìƒì„± í•¨ìˆ˜
# =========================

def get_headers(referer_url: str = None) -> dict:
    """
    ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ëŠ” í—¤ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        referer_url: Referer í—¤ë”ì— ì‚¬ìš©í•  URL (ê¸°ë³¸ê°’: KMOU_MAIN_URL)
    
    Returns:
        í—¤ë” ë”•ì…”ë„ˆë¦¬
    """
    headers = {
        'User-Agent': LATEST_CHROME_UA,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Referer': referer_url or KMOU_MAIN_URL,
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'same-origin',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
    }
    return headers

# =========================
# ìš°ì•„í•œ ì‹¤íŒ¨ ë©”ì‹œì§€
# =========================

FALLBACK_MESSAGE = (
    "âš ï¸ í˜„ì¬ í•™êµ í™ˆí˜ì´ì§€ ë³´ì•ˆ ì ê²€ìœ¼ë¡œ ì¸í•´ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
    "ì •í™•í•œ ë‚´ìš©ì€ í•™êµ í™ˆí˜ì´ì§€ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”."
)

# =========================
# ê³„ì¸µì  ì˜ˆì™¸ ì²˜ë¦¬ í•¨ìˆ˜
# =========================

def save_fallback_message(file_path: Path, url: str = None):
    """
    ì‹¤íŒ¨ ì‹œ ì¹œì ˆí•œ ì•ˆë‚´ ë¬¸êµ¬ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        file_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        url: ì›ë³¸ URL (ì„ íƒ)
    """
    try:
        message = FALLBACK_MESSAGE
        if url:
            message += f"\n\nì›ë³¸ URL: {url}"
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(message)
        logger.info(f"ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥ ì™„ë£Œ: {file_path.name}")
    except Exception as e:
        logger.error(f"ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥ ì‹¤íŒ¨: {e}")

# =========================
# ì•ˆì „í•œ ìš”ì²­ í•¨ìˆ˜
# =========================

def safe_request(url: str, filename: str, max_retries: int = 2):
    """
    ë³´ì•ˆ ìš°íšŒ ë° ì˜ˆì™¸ ì²˜ë¦¬ê°€ ê°•í™”ëœ ì•ˆì „í•œ HTTP ìš”ì²­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        url: ìš”ì²­í•  URL
        filename: ë¡œê·¸ìš© íŒŒì¼ëª…
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    
    Returns:
        response ê°ì²´ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    headers = get_headers()
    
    for attempt in range(max_retries + 1):
        try:
            # ìš”ì²­ ì „ ëœë¤ ë”œë ˆì´ (ì¸ê°„ì˜ ì ‘ì† íŒ¨í„´ ëª¨ì‚¬)
            if attempt > 0:
                delay = random.uniform(2, 4)  # ì¬ì‹œë„ ì‹œ ë” ê¸´ ë”œë ˆì´
                logger.info(f"[{filename}] ì¬ì‹œë„ ì „ ëŒ€ê¸°: {delay:.2f}ì´ˆ")
                time.sleep(delay)
            else:
                delay = random.uniform(1, 3)
                time.sleep(delay)
            
            logger.info(f"[{filename}] ìš”ì²­ ì‹œë„ {attempt + 1}/{max_retries + 1}: {url}")
            
            # CloudScraperë¡œ ìš”ì²­ (ì±Œë¦°ì§€ ìë™ ìš°íšŒ, ì„¸ì…˜ìœ¼ë¡œ ì¿ í‚¤ ìœ ì§€)
            response = scraper_session.get(url, headers=headers, timeout=30)
            
            # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
            if response.status_code == 403:
                logger.warning(f"[{filename}] 403 Forbidden ë°œìƒ")
                return None
            elif response.status_code == 404:
                logger.warning(f"[{filename}] 404 Not Found ë°œìƒ")
                return None
            elif response.status_code != 200:
                logger.warning(f"[{filename}] HTTP {response.status_code} ë°œìƒ")
                if attempt < max_retries:
                    continue
                return None
            
            response.raise_for_status()
            logger.info(f"[{filename}] ìš”ì²­ ì„±ê³µ (ìƒíƒœ ì½”ë“œ: {response.status_code})")
            return response
            
        except cloudscraper.exceptions.CloudflareChallengeError as e:
            logger.error(f"[{filename}] Cloudflare ì±Œë¦°ì§€ ì‹¤íŒ¨: {e}")
            if attempt < max_retries:
                logger.info(f"[{filename}] ì¬ì‹œë„ ì˜ˆì •...")
                continue
            return None
            
        except Exception as e:
            # requests.exceptions.Timeout í¬í•¨
            error_type = type(e).__name__
            logger.error(f"[{filename}] ìš”ì²­ ì‹¤íŒ¨ ({error_type}): {e}")
            if attempt < max_retries:
                logger.info(f"[{filename}] ì¬ì‹œë„ ì˜ˆì •...")
                continue
            return None
    
    return None

# =========================
# ì•ˆì „í•œ HTML íŒŒì‹± í•¨ìˆ˜
# =========================

def safe_parse_html(response_text: str, filename: str) -> str:
    """
    HTMLì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        response_text: HTML í…ìŠ¤íŠ¸
        filename: ë¡œê·¸ìš© íŒŒì¼ëª…
    
    Returns:
        íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´
    """
    try:
        soup = BeautifulSoup(response_text, 'html.parser')
        content = soup.get_text(separator='\n', strip=True)
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê°„ì£¼
        if len(content) < 50:
            logger.warning(f"[{filename}] íŒŒì‹±ëœ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ ({len(content)}ì)")
            return ""
        
        logger.info(f"[{filename}] HTML íŒŒì‹± ì„±ê³µ ({len(content)}ì)")
        return content
        
    except AttributeError as e:
        logger.error(f"[{filename}] HTML êµ¬ì¡° íŒŒì‹± ì‹¤íŒ¨ (AttributeError): {e}")
        return ""
    except IndexError as e:
        logger.error(f"[{filename}] HTML êµ¬ì¡° íŒŒì‹± ì‹¤íŒ¨ (IndexError): {e}")
        return ""
    except Exception as e:
        logger.error(f"[{filename}] HTML íŒŒì‹± ì‹¤íŒ¨: {e}")
        return ""

# =========================
# ì‹ë‹¨ ì •ë³´ ìˆ˜ì§‘ í•¨ìˆ˜
# =========================

def fetch_meal(url: str, file_path: Path) -> bool:
    """
    ì‹ë‹¨ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        url: ì‹ë‹¨ í˜ì´ì§€ URL
        file_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    logger.info(f"[ì‹ë‹¨] ìˆ˜ì§‘ ì‹œì‘: {url}")
    
    try:
        response = safe_request(url, "ì‹ë‹¨")
        
        if response is None:
            save_fallback_message(file_path, url)
            return False
        
        # HTML íŒŒì‹±
        content = safe_parse_html(response.text, "ì‹ë‹¨")
        
        if not content:
            logger.warning("[ì‹ë‹¨] íŒŒì‹±ëœ ë‚´ìš©ì´ ì—†ìŒ")
            save_fallback_message(file_path, url)
            return False
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        logger.info(f"[ì‹ë‹¨] ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨: {file_path.name}")
        return True
        
    except Exception as e:
        logger.error(f"[ì‹ë‹¨] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        save_fallback_message(file_path, url)
        return False

# =========================
# ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ í•¨ìˆ˜
# =========================

def fetch_notice(url: str, file_path: Path) -> bool:
    """
    ê³µì§€ì‚¬í•­ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        url: ê³µì§€ì‚¬í•­ í˜ì´ì§€ URL
        file_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    logger.info(f"[ê³µì§€ì‚¬í•­] ìˆ˜ì§‘ ì‹œì‘: {url}")
    
    try:
        response = safe_request(url, "ê³µì§€ì‚¬í•­")
        
        if response is None:
            save_fallback_message(file_path, url)
            return False
        
        # HTML íŒŒì‹±
        content = safe_parse_html(response.text, "ê³µì§€ì‚¬í•­")
        
        if not content:
            logger.warning("[ê³µì§€ì‚¬í•­] íŒŒì‹±ëœ ë‚´ìš©ì´ ì—†ìŒ")
            save_fallback_message(file_path, url)
            return False
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        logger.info(f"[ê³µì§€ì‚¬í•­] ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨: {file_path.name}")
        return True
        
    except Exception as e:
        logger.error(f"[ê³µì§€ì‚¬í•­] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        save_fallback_message(file_path, url)
        return False

# =========================
# í•™ì‚¬ ì•ˆë‚´ ìˆ˜ì§‘ í•¨ìˆ˜
# =========================

def fetch_academic_guide(url: str, file_path: Path) -> bool:
    """
    í•™ì‚¬ ì•ˆë‚´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        url: í•™ì‚¬ ì•ˆë‚´ í˜ì´ì§€ URL
        file_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    logger.info(f"[í•™ì‚¬ì•ˆë‚´] ìˆ˜ì§‘ ì‹œì‘: {url}")
    
    try:
        response = safe_request(url, "í•™ì‚¬ì•ˆë‚´")
        
        if response is None:
            save_fallback_message(file_path, url)
            return False
        
        # HTML íŒŒì‹±
        content = safe_parse_html(response.text, "í•™ì‚¬ì•ˆë‚´")
        
        if not content:
            logger.warning("[í•™ì‚¬ì•ˆë‚´] íŒŒì‹±ëœ ë‚´ìš©ì´ ì—†ìŒ")
            save_fallback_message(file_path, url)
            return False
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        logger.info(f"[í•™ì‚¬ì•ˆë‚´] ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨: {file_path.name}")
        return True
        
    except Exception as e:
        logger.error(f"[í•™ì‚¬ì•ˆë‚´] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        save_fallback_message(file_path, url)
        return False

# =========================
# ì¥í•™ê¸ˆ ì•ˆë‚´ ìˆ˜ì§‘ í•¨ìˆ˜
# =========================

def fetch_scholarship_guide(url: str, file_path: Path) -> bool:
    """
    ì¥í•™ê¸ˆ ì•ˆë‚´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        url: ì¥í•™ê¸ˆ ì•ˆë‚´ í˜ì´ì§€ URL
        file_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    logger.info(f"[ì¥í•™ê¸ˆ] ìˆ˜ì§‘ ì‹œì‘: {url}")
    
    try:
        response = safe_request(url, "ì¥í•™ê¸ˆ")
        
        if response is None:
            save_fallback_message(file_path, url)
            return False
        
        # HTML íŒŒì‹±
        content = safe_parse_html(response.text, "ì¥í•™ê¸ˆ")
        
        if not content:
            logger.warning("[ì¥í•™ê¸ˆ] íŒŒì‹±ëœ ë‚´ìš©ì´ ì—†ìŒ")
            save_fallback_message(file_path, url)
            return False
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        logger.info(f"[ì¥í•™ê¸ˆ] ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨: {file_path.name}")
        return True
        
    except Exception as e:
        logger.error(f"[ì¥í•™ê¸ˆ] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        save_fallback_message(file_path, url)
        return False

# =========================
# í–‰ì‚¬/ì„¸ë¯¸ë‚˜ ìˆ˜ì§‘ í•¨ìˆ˜
# =========================

def fetch_events_seminar(url: str, file_path: Path) -> bool:
    """
    í–‰ì‚¬/ì„¸ë¯¸ë‚˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        url: í–‰ì‚¬/ì„¸ë¯¸ë‚˜ í˜ì´ì§€ URL
        file_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    logger.info(f"[í–‰ì‚¬/ì„¸ë¯¸ë‚˜] ìˆ˜ì§‘ ì‹œì‘: {url}")
    
    try:
        response = safe_request(url, "í–‰ì‚¬/ì„¸ë¯¸ë‚˜")
        
        if response is None:
            save_fallback_message(file_path, url)
            return False
        
        # HTML íŒŒì‹±
        content = safe_parse_html(response.text, "í–‰ì‚¬/ì„¸ë¯¸ë‚˜")
        
        if not content:
            logger.warning("[í–‰ì‚¬/ì„¸ë¯¸ë‚˜] íŒŒì‹±ëœ ë‚´ìš©ì´ ì—†ìŒ")
            save_fallback_message(file_path, url)
            return False
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        logger.info(f"[í–‰ì‚¬/ì„¸ë¯¸ë‚˜] ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨: {file_path.name}")
        return True
        
    except Exception as e:
        logger.error(f"[í–‰ì‚¬/ì„¸ë¯¸ë‚˜] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        save_fallback_message(file_path, url)
        return False

# =========================
# ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜ (í†µí•©)
# =========================

def collect_university_info(target_url: str, filename: str) -> bool:
    """
    í•™êµ í™ˆí˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    ë°©íƒ„ ëª¨ë“œ: ë³´ì•ˆ ìš°íšŒ ì‹¬í™” + ê³„ì¸µì  ì˜ˆì™¸ ì²˜ë¦¬ + ìš°ì•„í•œ ì‹¤íŒ¨
    
    Args:
        target_url: ìˆ˜ì§‘í•  URL
        filename: ì €ì¥í•  íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    file_path = data_dir / f"{filename}.txt"
    
    try:
        # íŒŒì¼ëª…ì— ë”°ë¼ ì ì ˆí•œ í•¨ìˆ˜ í˜¸ì¶œ
        if filename == "cafeteria_menu":
            return fetch_meal(target_url, file_path)
        elif filename == "notice_general":
            return fetch_notice(target_url, file_path)
        elif filename == "academic_guide":
            return fetch_academic_guide(target_url, file_path)
        elif filename == "scholarship_guide":
            return fetch_scholarship_guide(target_url, file_path)
        elif filename == "events_seminar":
            return fetch_events_seminar(target_url, file_path)
        else:
            # ê¸°ë³¸ ì²˜ë¦¬ (ì¼ë°˜ í˜ì´ì§€)
            logger.info(f"[{filename}] ìˆ˜ì§‘ ì‹œì‘: {target_url}")
            response = safe_request(target_url, filename)
            
            if response is None:
                save_fallback_message(file_path, target_url)
                return False
            
            content = safe_parse_html(response.text, filename)
            
            if not content:
                save_fallback_message(file_path, target_url)
                return False
            
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content)
            
            logger.info(f"[{filename}] ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨: {file_path.name}")
            return True
            
    except Exception as e:
        logger.error(f"[{filename}] ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        save_fallback_message(file_path, target_url)
        return False

# =========================
# ë©”ì¸ ì‹¤í–‰ ë¡œì§
# =========================

if __name__ == "__main__":
    try:
        logger.info("=" * 60)
        logger.info("ë°©íƒ„ ëª¨ë“œ ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹œì‘")
        logger.info("=" * 60)
        
        # KMOU ê²Œì‹œíŒ ëª©ë¡
        urls_to_crawl = {
            "notice_general": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2032&bbsId=10373",
            "academic_guide": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2033&bbsId=11786",
            "scholarship_guide": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=5691&bbsId=10004365",
            "events_seminar": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2034&bbsId=10375",
            "cafeteria_menu": "https://www.kmou.ac.kr/coop/dv/dietView/selectDietDateView.do?mi=1189"
        }
        
        success_count = 0
        total_count = len(urls_to_crawl)
        
        for name, url in urls_to_crawl.items():
            try:
                if collect_university_info(url, name):
                    success_count += 1
            except Exception as e:
                logger.error(f"[{name}] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                # íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥
                file_path = data_dir / f"{name}.txt"
                save_fallback_message(file_path, url)
            
            # ìš”ì²­ ì‚¬ì´ ëœë¤ ë”œë ˆì´ (ì¸ê°„ì˜ ì ‘ì† íŒ¨í„´ ëª¨ì‚¬)
            if name != list(urls_to_crawl.keys())[-1]:  # ë§ˆì§€ë§‰ í•­ëª©ì´ ì•„ë‹ˆë©´
                delay = random.uniform(1, 3)
                logger.info(f"ë‹¤ìŒ ìš”ì²­ ì „ ëŒ€ê¸°: {delay:.2f}ì´ˆ")
                time.sleep(delay)
        
        logger.info("=" * 60)
        logger.info(f"ìˆ˜ì§‘ ì‘ì—… ì™„ë£Œ: {success_count}/{total_count} ì„±ê³µ")
        logger.info("=" * 60)
        print(f"\nğŸš€ ëª¨ë“  ìˆ˜ì§‘ ì‘ì—…ì´ ëë‚¬ìŠµë‹ˆë‹¤. (ì„±ê³µ: {success_count}/{total_count})")
        print("ì´ì œ ingest.pyë¥¼ ì‹¤í–‰í•´ ë³´ì„¸ìš”!")
        
    except Exception as e:
        # ì „ì²´ í”„ë¡œê·¸ë¨ì´ í¬ë˜ì‹œë˜ì§€ ì•Šë„ë¡ ìµœìƒìœ„ ì˜ˆì™¸ ì²˜ë¦¬
        logger.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        print(f"\nâš ï¸ ìˆ˜ì§‘ ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        print("ë¡œê·¸ íŒŒì¼(collector.log)ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        # Exit code 0ìœ¼ë¡œ ì •ìƒ ì¢…ë£Œ (ì„œë²„ ì‹¤í–‰ì´ ë§‰íˆì§€ ì•Šë„ë¡)
        sys.exit(0)
