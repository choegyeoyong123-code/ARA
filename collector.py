"""
ë°©íƒ„ ëª¨ë“œ ë°ì´í„° ìˆ˜ì§‘ê¸° (Bulletproof Collector)
- Import ì—ëŸ¬ ë°©ì–´: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì–´ë„ ì„œë²„ ê¸°ë™ ë³´ì¥
- ë³´ì•ˆ ìš°íšŒ ì‹¬í™”: ì„¸ì…˜ ìœ ì§€, ëœë¤ ë”œë ˆì´
- ê³„ì¸µì  ì˜ˆì™¸ ì²˜ë¦¬: ëª¨ë“  ì—ëŸ¬ ìƒí™© ëŒ€ì‘
- ìš°ì•„í•œ ì‹¤íŒ¨: í¬ë˜ì‹œ ì—†ì´ ì¹œì ˆí•œ ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥
"""

import os
import sys
import time
import random
import logging
from pathlib import Path

# =================================================================
# [í•µì‹¬] ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë°©ì–´ë§‰ (Import Errorë¡œ ì¸í•œ Crash ë°©ì§€)
# =================================================================
try:
    from dotenv import load_dotenv
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    import cloudscraper
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"âš ï¸ [Critical] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("â¡ï¸ í¬ë¡¤ë§ì„ ê±´ë„ˆë›°ê³  ì •ìƒ ì¢…ë£Œí•©ë‹ˆë‹¤. (ì„œë²„ ì‹¤í–‰ ë³´ì¥)")
    # ì—¬ê¸°ì„œ Exit 1ì„ ë‚´ë©´ ì„œë²„ê°€ ì£½ìŠµë‹ˆë‹¤. Exit 0ìœ¼ë¡œ ì†ì—¬ì„œ ì„œë²„ë¥¼ ì‚´ë¦½ë‹ˆë‹¤.
    sys.exit(0)

# =========================
# ë¡œê¹… ì„¤ì •
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler() # íŒŒì¼ ë¡œê¹… ì œê±° (Render ë””ìŠ¤í¬ ê¶Œí•œ ë¬¸ì œ ë°©ì§€)
    ]
)
logger = logging.getLogger(__name__)

# =========================
# ì „ì—­ ì„¤ì •
# =========================

# í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ì ¸ì™€ì„œ 'university_data' í´ë” ê²½ë¡œ í™•ì •
current_dir = Path(__file__).parent.absolute()
data_dir = current_dir / "university_data"

# í•™êµ í™ˆí˜ì´ì§€ ë©”ì¸ ì£¼ì†Œ
KMOU_MAIN_URL = "https://www.kmou.ac.kr"

# ìµœì‹  Chrome User-Agent
LATEST_CHROME_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/131.0.0.0 Safari/537.36"
)

# =========================
# CloudScraper ì„¸ì…˜ ì´ˆê¸°í™”
# =========================

def create_scraper_session():
    try:
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            },
            delay=random.uniform(1, 2)
        )
        return scraper
    except Exception as e:
        logger.error(f"CloudScraper ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# ì „ì—­ ì„¸ì…˜ (ì‹¤íŒ¨ ì‹œ None)
scraper_session = create_scraper_session()

# =========================
# í—¤ë” ìƒì„± í•¨ìˆ˜
# =========================

def get_headers(referer_url: str = None) -> dict:
    headers = {
        'User-Agent': LATEST_CHROME_UA,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Referer': referer_url or KMOU_MAIN_URL,
        'Upgrade-Insecure-Requests': '1',
    }
    return headers

# =========================
# ìš°ì•„í•œ ì‹¤íŒ¨ ì²˜ë¦¬
# =========================

FALLBACK_MESSAGE = (
    "âš ï¸ í˜„ì¬ í•™êµ í™ˆí˜ì´ì§€ ë³´ì•ˆ ì ê²€ ë˜ëŠ” ì—°ê²° ë¬¸ì œë¡œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
    "ì •í™•í•œ ë‚´ìš©ì€ í•™êµ í™ˆí˜ì´ì§€ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”."
)

def save_fallback_message(file_path: Path, url: str = None):
    try:
        # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„± (ë°©ì–´ ì½”ë“œ)
        if not file_path.parent.exists():
            file_path.parent.mkdir(parents=True, exist_ok=True)

        message = FALLBACK_MESSAGE
        if url:
            message += f"\n\nì›ë³¸ URL: {url}"
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(message)
        logger.info(f"ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥ ì™„ë£Œ: {file_path.name}")
    except Exception as e:
        logger.error(f"ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥ ì‹¤íŒ¨: {e}")

# =========================
# ì•ˆì „í•œ ìš”ì²­ í•¨ìˆ˜
# =========================

def safe_request(url: str, filename: str):
    if scraper_session is None:
        return None

    headers = get_headers()
    
    # ìµœëŒ€ 2íšŒ ì¬ì‹œë„
    for attempt in range(2):
        try:
            if attempt > 0:
                time.sleep(random.uniform(2, 4))
            
            response = scraper_session.get(url, headers=headers, timeout=15) # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶• (ì„œë²„ ì§€ì—° ë°©ì§€)
            
            if response.status_code == 200:
                return response
            elif response.status_code in [403, 404]:
                logger.warning(f"[{filename}] HTTP {response.status_code}")
                return None
            
        except Exception as e:
            logger.error(f"[{filename}] ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue
            
    return None

# =========================
# ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜ (ë‹¨ìˆœí™”)
# =========================

def collect_and_save(url: str, filename: str):
    file_path = data_dir / f"{filename}.txt"
    
    try:
        response = safe_request(url, filename)
        
        if response and len(response.text) > 100:
            # HTML íŒŒì‹± (BeautifulSoup)
            soup = BeautifulSoup(response.text, 'html.parser')
            content = soup.get_text(separator='\n', strip=True)
            
            if len(content) > 50:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(content)
                logger.info(f"âœ… [{filename}] ì €ì¥ ì„±ê³µ")
                return True
                
    except Exception as e:
        logger.error(f"[{filename}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # ì‹¤íŒ¨ ì‹œ ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥
    save_fallback_message(file_path, url)
    return False

# =========================
# ë©”ì¸ ì‹¤í–‰ ë¡œì§
# =========================

def main():
    print("ğŸš€ [Collector] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    # í´ë” ìƒì„±
    if not data_dir.exists():
        data_dir.mkdir(parents=True, exist_ok=True)

    urls = {
        "notice_general": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2032&bbsId=10373",
        "academic_guide": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2033&bbsId=11786",
        "scholarship_guide": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=5691&bbsId=10004365",
        "cafeteria_menu": "https://www.kmou.ac.kr/coop/dv/dietView/selectDietDateView.do?mi=1189"
    }

    for name, url in urls.items():
        collect_and_save(url, name)
        time.sleep(1) # ë¶€í•˜ ë°©ì§€

    print("âœ… [Collector] ëª¨ë“  ì‘ì—… ì™„ë£Œ. ì •ìƒ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        # ì–´ë–¤ ì¹˜ëª…ì  ì˜¤ë¥˜ê°€ ë‚˜ë„ ë¡œê·¸ë§Œ ì°ê³  ì •ìƒ ì¢…ë£Œ(Exit 0)
        print(f"âš ï¸ [Collector] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("â¡ï¸ ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ìœ„í•´ ì •ìƒ ì¢…ë£Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    finally:
        sys.exit(0) # <--- [í•µì‹¬] ë¬´ì¡°ê±´ ì„±ê³µí•œ ì²™ ì¢…ë£Œ