import cloudscraper
import os
from pathlib import Path
from bs4 import BeautifulSoup
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# 1. í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ì ¸ì™€ì„œ 'university_data' í´ë” ê²½ë¡œ í™•ì •
# OneDrive í™˜ê²½ì—ì„œë„ ê°€ì¥ ì•ˆì „í•œ pathlibì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
current_dir = Path(__file__).parent.absolute()
data_dir = current_dir / "university_data"

# 2. í´ë” ê°•ì œ ìƒì„± ë° ê¶Œí•œ í™•ì¸
try:
    data_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ… ë°ì´í„° ì €ì¥ ê²½ë¡œ í™•ë³´: {data_dir}")
except Exception as e:
    print(f"âš ï¸ í´ë” ìƒì„± ì¤‘ ê²½ê³  (ë¬´ì‹œ ê°€ëŠ¥): {e}")

# 3. CloudScraper ì´ˆê¸°í™” (Chrome ë¸Œë¼ìš°ì €ë¡œ ìœ„ì¥)
scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'chrome',
        'platform': 'windows',
        'mobile': False
    }
)

# í•™êµ í™ˆí˜ì´ì§€ ë©”ì¸ ì£¼ì†Œ
KMOU_MAIN_URL = "https://www.kmou.ac.kr"

def collect_university_info(target_url, filename):
    """
    í•™êµ í™ˆí˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    ë³´ì•ˆ ê°•í™”ë¡œ ì¸í•œ ì°¨ë‹¨ì„ ìš°íšŒí•˜ê¸° ìœ„í•´ cloudscraperë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ“¡ ë°ì´í„° ìˆ˜ì§‘ ì¤‘: [{filename}] ...")
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    file_path = data_dir / f"{filename}.txt"
    
    try:
        # í—¤ë” ì„¤ì • (ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ë„ë¡)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': KMOU_MAIN_URL,
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
        }
        
        # CloudScraperë¡œ ìš”ì²­ (ì±Œë¦°ì§€ ìë™ ìš°íšŒ)
        response = scraper.get(target_url, headers=headers, timeout=30)
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
        if response.status_code == 403:
            error_msg = f"í˜„ì¬ ë³´ì•ˆ ì ê²€ìœ¼ë¡œ ì¸í•´ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  ë‹¤ìŒ ë§í¬ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”: {target_url}"
            logger.warning(f"403 Forbidden ë°œìƒ ({filename}): {target_url}")
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(error_msg)
            print(f"  âš ï¸ 403 ì—ëŸ¬ - ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥: {file_path.name}")
            return
        
        if response.status_code == 404:
            error_msg = f"í˜„ì¬ ë³´ì•ˆ ì ê²€ìœ¼ë¡œ ì¸í•´ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  ë‹¤ìŒ ë§í¬ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”: {target_url}"
            logger.warning(f"404 Not Found ë°œìƒ ({filename}): {target_url}")
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(error_msg)
            print(f"  âš ï¸ 404 ì—ëŸ¬ - ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥: {file_path.name}")
            return
        
        # ì‘ë‹µ ìƒíƒœ ì½”ë“œ í™•ì¸
        response.raise_for_status()
        
        # ì‹ë‹¨ í˜ì´ì§€ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if filename == "cafeteria_menu":
            try:
                # HTML íŒŒì‹±
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì‹ë‹¨ ì •ë³´ ì¶”ì¶œ ì‹œë„
                # ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
                content = soup.get_text(separator='\n', strip=True)
                
                # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê°„ì£¼
                if len(content) < 100:
                    raise ValueError("ì‹ë‹¨ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                
                # íŒŒì¼ ì €ì¥
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(content)
                
                print(f"  âœ¨ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨: {file_path.name}")
                
            except Exception as parse_error:
                error_msg = "ì‹ë‹¨ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                logger.error(f"ì‹ë‹¨ í˜ì´ì§€ íŒŒì‹± ì‹¤íŒ¨ ({filename}): {parse_error}")
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(f"{error_msg}\nì›ë³¸ URL: {target_url}")
                print(f"  âš ï¸ íŒŒì‹± ì‹¤íŒ¨ - ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥: {file_path.name}")
        else:
            # ì¼ë°˜ í˜ì´ì§€ ì²˜ë¦¬
            # HTMLì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            soup = BeautifulSoup(response.text, 'html.parser')
            content = soup.get_text(separator='\n', strip=True)
            
            # íŒŒì¼ ì €ì¥
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content)
            
            print(f"  âœ¨ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨: {file_path.name}")
        
    except Exception as e:
        # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬ (í”„ë¡œê·¸ë¨ ì¤‘ë‹¨í•˜ì§€ ì•Šê³  ë¡œê·¸ë§Œ ë‚¨ê¹€)
        error_msg = f"í˜„ì¬ ë³´ì•ˆ ì ê²€ìœ¼ë¡œ ì¸í•´ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  ë‹¤ìŒ ë§í¬ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”: {target_url}"
        logger.error(f"ìˆ˜ì§‘ ì‹¤íŒ¨ ({filename}): {e}")
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(error_msg)
        print(f"  âŒ ìˆ˜ì§‘ ì‹¤íŒ¨ - ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥: {file_path.name}")

# --- KMOU ê²Œì‹œíŒ ëª©ë¡ ---
# --- ìˆ˜ì§‘í•  ê²Œì‹œíŒ ëª©ë¡ì— 'ì‹ë‹¨ ì •ë³´' ì¶”ê°€ ---
urls_to_crawl = {
    "notice_general": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2032&bbsId=10373",
    "academic_guide": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2033&bbsId=11786",
    "scholarship_guide": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=5691&bbsId=10004365",
    "events_seminar": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2034&bbsId=10375",
    "cafeteria_menu": "https://www.kmou.ac.kr/coop/dv/dietView/selectDietDateView.do?mi=1189" # ì¶”ê°€ëœ ì‹ë‹¨ URL
}
for name, url in urls_to_crawl.items():
    collect_university_info(url, name)

print("\nğŸš€ ëª¨ë“  ìˆ˜ì§‘ ì‘ì—…ì´ ëë‚¬ìŠµë‹ˆë‹¤. ì´ì œ ingest.pyë¥¼ ì‹¤í–‰í•´ ë³´ì„¸ìš”!")