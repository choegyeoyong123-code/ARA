"""
ë°©íƒ„ ëª¨ë“œ ë°ì´í„° ìˆ˜ì§‘ê¸° (Bulletproof Collector)
- Import ì—ëŸ¬ ë°©ì–´: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì–´ë„ ì„œë²„ ê¸°ë™ ë³´ì¥
- ë³´ì•ˆ ìš°íšŒ ì‹¬í™”: ì„¸ì…˜ ìœ ì§€, ëœë¤ ë”œë ˆì´
- ê³„ì¸µì  ì˜ˆì™¸ ì²˜ë¦¬: ëª¨ë“  ì—ëŸ¬ ìƒí™© ëŒ€ì‘
- ìš°ì•„í•œ ì‹¤íŒ¨: í¬ë˜ì‹œ ì—†ì´ ì¹œì ˆí•œ ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥
"""

import os
import sys
import time
import random
import logging
from pathlib import Path

# =================================================================
# [í•µì‹¬] ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë°©ì–´ë§‰ (Import Errorë¡œ ì¸í•œ Crash ë°©ì§€)
# =================================================================
try:
    from dotenv import load_dotenv
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    import cloudscraper
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"âš ï¸ [Critical] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("â¡ï¸ í¬ë¡¤ë§ì„ ê±´ë„ˆë›°ê³  ì •ìƒ ì¢…ë£Œí•©ë‹ˆë‹¤. (ì„œë²„ ì‹¤í–‰ ë³´ì¥)")
    # ì—¬ê¸°ì„œ Exit 1ì„ ë‚´ë©´ ì„œë²„ê°€ ì£½ìŠµë‹ˆë‹¤. Exit 0ìœ¼ë¡œ ì†ì—¬ì„œ ì„œë²„ë¥¼ ì‚´ë¦½ë‹ˆë‹¤.
    sys.exit(0)

# =========================
# ë¡œê¹… ì„¤ì •
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler() # íŒŒì¼ ë¡œê¹… ì œê±° (Render ë””ìŠ¤í¬ ê¶Œí•œ ë¬¸ì œ ë°©ì§€)
    ]
)
logger = logging.getLogger(__name__)

# =========================
# ì „ì—­ ì„¤ì •
# =========================

# í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ì ¸ì™€ì„œ 'university_data' í´ë” ê²½ë¡œ í™•ì •
current_dir = Path(__file__).parent.absolute()
data_dir = current_dir / "university_data"

# í•™êµ í™ˆí˜ì´ì§€ ë©”ì¸ ì£¼ì†Œ
KMOU_MAIN_URL = "https://www.kmou.ac.kr"

# ìµœì‹  Chrome User-Agent
LATEST_CHROME_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/131.0.0.0 Safari/537.36"
)

# =========================
# CloudScraper ì„¸ì…˜ ì´ˆê¸°í™”
# =========================

def create_scraper_session():
    """
    ë³´ì•ˆ ìš°íšŒ ê°•í™”ëœ CloudScraper ì„¸ì…˜ ìƒì„±
    - ìµœì‹  Chrome ë¸Œë¼ìš°ì € ëª¨ì‚¬
    - ì¿ í‚¤ ë° ì„¸ì…˜ ìœ ì§€
    - ìë™ ì±Œë¦°ì§€ ìš°íšŒ
    """
    try:
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False,
                'desktop': True
            },
            delay=random.uniform(1.5, 3.0),  # ë”œë ˆì´ ì¦ê°€ (ì¸ê°„ íŒ¨í„´)
            debug=False
        )
        
        # ì¶”ê°€ í—¤ë” ì„¤ì • (ë¸Œë¼ìš°ì € ëª¨ì‚¬ ê°•í™”)
        scraper.headers.update({
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
        })
        
        logger.info("âœ… CloudScraper ì„¸ì…˜ ìƒì„± ì™„ë£Œ (ë³´ì•ˆ ìš°íšŒ í™œì„±í™”)")
        return scraper
    except Exception as e:
        logger.error(f"âŒ CloudScraper ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# ì „ì—­ ì„¸ì…˜ (ì‹¤íŒ¨ ì‹œ None)
scraper_session = create_scraper_session()

# =========================
# í—¤ë” ìƒì„± í•¨ìˆ˜
# =========================

def get_headers(referer_url: str = None) -> dict:
    headers = {
        'User-Agent': LATEST_CHROME_UA,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Referer': referer_url or KMOU_MAIN_URL,
        'Upgrade-Insecure-Requests': '1',
    }
    return headers

# =========================
# ìš°ì•„í•œ ì‹¤íŒ¨ ì²˜ë¦¬
# =========================

FALLBACK_MESSAGE = (
    "âš ï¸ í˜„ì¬ í•™êµ í™ˆí˜ì´ì§€ ë³´ì•ˆ ì ê²€ ë˜ëŠ” ì—°ê²° ë¬¸ì œë¡œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
    "ì •í™•í•œ ë‚´ìš©ì€ í•™êµ í™ˆí˜ì´ì§€ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”."
)

def save_fallback_message(file_path: Path, url: str = None):
    try:
        # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„± (ë°©ì–´ ì½”ë“œ)
        if not file_path.parent.exists():
            file_path.parent.mkdir(parents=True, exist_ok=True)

        message = FALLBACK_MESSAGE
        if url:
            message += f"\n\nì›ë³¸ URL: {url}"
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(message)
        logger.info(f"ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥ ì™„ë£Œ: {file_path.name}")
    except Exception as e:
        logger.error(f"ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥ ì‹¤íŒ¨: {e}")

# =========================
# ì•ˆì „í•œ ìš”ì²­ í•¨ìˆ˜
# =========================

def safe_request(url: str, filename: str):
    """
    ë³´ì•ˆ ìš°íšŒ ê°•í™”ëœ ì•ˆì „í•œ ìš”ì²­ í•¨ìˆ˜
    - Cloudflare ì±Œë¦°ì§€ ìš°íšŒ
    - ì„¸ì…˜ ì¿ í‚¤ ìœ ì§€
    - ê³„ì¸µì  ì˜ˆì™¸ ì²˜ë¦¬
    """
    if scraper_session is None:
        return None

    headers = get_headers()
    
    # ìµœëŒ€ 3íšŒ ì¬ì‹œë„ (ë³´ì•ˆ ìš°íšŒ ê°•í™”)
    for attempt in range(3):
        try:
            if attempt > 0:
                # ì¬ì‹œë„ ì‹œ ë” ê¸´ ë”œë ˆì´ (ì¸ê°„ íŒ¨í„´ ëª¨ì‚¬)
                delay = random.uniform(3, 6)
                logger.info(f"[{filename}] ì¬ì‹œë„ {attempt}íšŒ - {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(delay)
            
            # ìš”ì²­ ì „ ëœë¤ ë”œë ˆì´ (ì¸ê°„ íŒ¨í„´)
            if attempt == 0:
                time.sleep(random.uniform(1, 3))
            
            # CloudScraperë¡œ ìš”ì²­ (ìë™ ì±Œë¦°ì§€ ìš°íšŒ)
            response = scraper_session.get(
                url, 
                headers=headers, 
                timeout=20,  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
                allow_redirects=True
            )
            
            if response.status_code == 200:
                # ì‘ë‹µ í¬ê¸° í™•ì¸ (ë„ˆë¬´ ì‘ìœ¼ë©´ ì˜ì‹¬)
                if len(response.text) < 100:
                    logger.warning(f"[{filename}] ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(response.text)}ì)")
                    if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„
                        continue
                
                return response
            elif response.status_code in [403, 404]:
                logger.warning(f"[{filename}] HTTP {response.status_code}")
                return None
            elif response.status_code == 429:  # Too Many Requests
                logger.warning(f"[{filename}] Rate Limit - ë” ê¸´ ëŒ€ê¸° í›„ ì¬ì‹œë„")
                if attempt < 2:
                    time.sleep(random.uniform(10, 15))
                    continue
                return None
            
        except Exception as e:
            error_type = type(e).__name__
            if "CloudflareChallengeError" in error_type or "Challenge" in str(e):
                logger.error(f"[{filename}] Cloudflare ì±Œë¦°ì§€ ì‹¤íŒ¨: {e}")
                if attempt < 2:
                    # ì±Œë¦°ì§€ ì‹¤íŒ¨ ì‹œ ë” ê¸´ ëŒ€ê¸°
                    time.sleep(random.uniform(5, 10))
                    continue
            elif "Timeout" in error_type:
                logger.error(f"[{filename}] íƒ€ì„ì•„ì›ƒ: {e}")
                if attempt < 2:
                    continue
            elif "AttributeError" in error_type or "IndexError" in error_type:
                logger.error(f"[{filename}] íŒŒì‹± ì˜¤ë¥˜: {e}")
                # íŒŒì‹± ì˜¤ë¥˜ëŠ” ì¬ì‹œë„ ë¶ˆí•„ìš”
                return None
            else:
                logger.error(f"[{filename}] ìš”ì²­ ì‹¤íŒ¨ ({error_type}): {e}")
                if attempt < 2:
                    continue
            
    return None

# =========================
# ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜ (ë‹¨ìˆœí™”)
# =========================

def collect_and_save(url: str, filename: str):
    """
    ë³´ì•ˆ ìš°íšŒ ê°•í™”ëœ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
    - ê³„ì¸µì  ì˜ˆì™¸ ì²˜ë¦¬
    - ìš°ì•„í•œ ì‹¤íŒ¨ ì²˜ë¦¬
    """
    file_path = data_dir / f"{filename}.txt"
    
    try:
        response = safe_request(url, filename)
        
        if response and len(response.text) > 100:
            try:
                # HTML íŒŒì‹± (BeautifulSoup) - lxml íŒŒì„œ ì‚¬ìš© (ë” ë¹ ë¥´ê³  ì•ˆì •ì )
                soup = BeautifulSoup(response.text, 'lxml')
                
                # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±° (ë…¸ì´ì¦ˆ ì œê±°)
                for script in soup(["script", "style", "meta", "link"]):
                    script.decompose()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                content = soup.get_text(separator='\n', strip=True)
                
                # ë¹ˆ ì¤„ ì œê±° ë° ì •ë¦¬
                lines = [line.strip() for line in content.split('\n') if line.strip()]
                content = '\n'.join(lines)
                
                if len(content) > 50:
                    # í´ë” ìƒì„± (ì•ˆì „ì¥ì¹˜)
                    file_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(content)
                    logger.info(f"âœ… [{filename}] ì €ì¥ ì„±ê³µ ({len(content)}ì)")
                    return True
                else:
                    logger.warning(f"[{filename}] ì¶”ì¶œëœ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(content)}ì)")
                    
            except AttributeError as e:
                logger.error(f"[{filename}] HTML êµ¬ì¡° íŒŒì‹± ì˜¤ë¥˜: {e}")
                # íŒŒì‹± ì˜¤ë¥˜ëŠ” íŠ¹ë³„ ë©”ì‹œì§€ ì €ì¥
                if filename == "cafeteria_menu":
                    fallback_msg = "ì‹ë‹¨ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                else:
                    fallback_msg = FALLBACK_MESSAGE
                save_fallback_message(file_path, url)
                return False
            except IndexError as e:
                logger.error(f"[{filename}] ì¸ë±ìŠ¤ ì˜¤ë¥˜ (HTML êµ¬ì¡° ë³€ê²½): {e}")
                save_fallback_message(file_path, url)
                return False
            except Exception as e:
                logger.error(f"[{filename}] íŒŒì‹± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                save_fallback_message(file_path, url)
                return False
        else:
            logger.warning(f"[{filename}] ì‘ë‹µì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
                
    except Exception as e:
        logger.error(f"[{filename}] ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())

    # ì‹¤íŒ¨ ì‹œ ì•ˆë‚´ ë¬¸êµ¬ ì €ì¥
    save_fallback_message(file_path, url)
    return False

# =========================
# ë©”ì¸ ì‹¤í–‰ ë¡œì§
# =========================

def main():
    print("ğŸš€ [Collector] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    # í´ë” ìƒì„±
    if not data_dir.exists():
        data_dir.mkdir(parents=True, exist_ok=True)

    urls = {
        "notice_general": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2032&bbsId=10373",
        "academic_guide": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2033&bbsId=11786",
        "scholarship_guide": "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=5691&bbsId=10004365",
        "cafeteria_menu": "https://www.kmou.ac.kr/coop/dv/dietView/selectDietDateView.do?mi=1189"
    }

    for name, url in urls.items():
        logger.info(f"ğŸ“¥ [{name}] ìˆ˜ì§‘ ì‹œì‘: {url}")
        collect_and_save(url, name)
        # ì¸ê°„ íŒ¨í„´ ëª¨ì‚¬: ê° ìš”ì²­ ì‚¬ì´ ëœë¤ ë”œë ˆì´
        delay = random.uniform(2, 4)
        time.sleep(delay)
        logger.info(f"â¸ï¸ [{name}] {delay:.1f}ì´ˆ ëŒ€ê¸° ì™„ë£Œ")

    print("âœ… [Collector] ëª¨ë“  ì‘ì—… ì™„ë£Œ. ì •ìƒ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        # ì–´ë–¤ ì¹˜ëª…ì  ì˜¤ë¥˜ê°€ ë‚˜ë„ ë¡œê·¸ë§Œ ì°ê³  ì •ìƒ ì¢…ë£Œ(Exit 0)
        print(f"âš ï¸ [Collector] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("â¡ï¸ ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ìœ„í•´ ì •ìƒ ì¢…ë£Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    finally:
        sys.exit(0) # <--- [í•µì‹¬] ë¬´ì¡°ê±´ ì„±ê³µí•œ ì²™ ì¢…ë£Œ